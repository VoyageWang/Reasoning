import re
import json
from scipy.optimize import linear_sum_assignment
import numpy as np

def vision_reasoner_format_reward(predict_str: str) -> float:
    pattern = r"<think>.*?</think>\s*<answer>.*?</answer>"
    match = re.fullmatch(pattern, predict_str, re.DOTALL)
    thinking_format_reward = 1.0 if match else 0.0 
    
    def segmentation_format(predict_str: str) -> float:
        segmentation_format_reward = 0.0
        try:
            json_match = re.search(r'<answer>\s*(.*?)\s*</answer>', predict_str, re.DOTALL)
            if not json_match:
                return segmentation_format_reward
            data = json.loads(json_match.group(1))
            
            data_cnt = len(data)
            
            for item in data:
                cur_reward = 0.0

                if 'bbox_2d' in item:
                    bbox_2d = item['bbox_2d']
                    if isinstance(bbox_2d, list) and len(bbox_2d) == 4:
                        cur_reward += 1.0
                    
                if 'point_2d' in item:
                    point_2d = item['point_2d']
                    if isinstance(point_2d, list) and len(point_2d) == 2:
                        cur_reward += 1.0
                
                segmentation_format_reward += cur_reward / data_cnt
        except Exception:
            pass
        return segmentation_format_reward
        
    segmentation_format_reward = segmentation_format(predict_str)
    
    return thinking_format_reward + segmentation_format_reward

def vision_reasoner_accuracy_reward(predict_str: str, ground_truth: str) -> float:
    max_accuracy_reward = 0.0
    MAX_OBJECTS = 120  # è®¾ç½®ä¸Šé™
    
    try:
        gt_data = json.loads(ground_truth)
        gt_bboxes = [item['bbox_2d'] for item in gt_data]
        gt_points = [item['point_2d'] for item in gt_data]
            
        #json_match = re.search(r'```json\s*(.*?)\s*```', predict_str, re.DOTALL)
        json_match = re.search(r'<answer>\s*(.*?)\s*</answer>', predict_str, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group(1))
            pred_bboxes = [item['bbox_2d'] for item in data]
            pred_points = [item['point_2d'] for item in data]
            
            # åªæœ‰å½“é¢„æµ‹æˆ–çœŸå®å€¼è¶…è¿‡ä¸Šé™æ—¶æ‰æˆªæ–­
            if len(pred_bboxes) > MAX_OBJECTS:
                pred_bboxes = pred_bboxes[:MAX_OBJECTS]
                pred_points = pred_points[:MAX_OBJECTS]
            
            if len(gt_bboxes) > MAX_OBJECTS:
                gt_bboxes = gt_bboxes[:MAX_OBJECTS]
                gt_points = gt_points[:MAX_OBJECTS]
            
            # é¢„å¤„ç†æ•°æ®ä¸ºnumpyæ•°ç»„
            pred_bboxes = np.array(pred_bboxes)  # (M,4)
            pred_points = np.array(pred_points)  # (M,2)
            gt_bboxes = np.array(gt_bboxes)    # (N,4)
            gt_points = np.array(gt_points)     # (N,2)
            
            # å¹¶è¡Œè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
            iou_matrix = batch_iou(pred_bboxes, gt_bboxes)  # (M,N)
            l1_matrix = batch_l1_distance(pred_bboxes, gt_bboxes)  # (M,N)
            points_dist_matrix = batch_points_distance(pred_points, gt_points)  # (M,N)
            points_in_box = batch_points_in_box(pred_points, pred_bboxes)  # (M,)
            
            # è®¡ç®—rewardçŸ©é˜µ
            iou_reward = (iou_matrix > 0.5).astype(float)
            bbox_l1_reward = (l1_matrix < 10).astype(float)
            point_reward = ((points_dist_matrix < 30) & points_in_box[:,np.newaxis]).astype(float)
            
            # æ„å»ºæœ€ç»ˆçš„costçŸ©é˜µ
            cost_matrix = 3.0 - (iou_reward + bbox_l1_reward + point_reward)
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ‰¾æœ€ä¼˜åŒ¹é…
            row_indices, col_indices = linear_sum_assignment(cost_matrix)
            
            # ç›´æ¥ä»cost_matrixè®¡ç®—æ€»reward
            total_reward = len(row_indices) * 3.0 - cost_matrix[row_indices, col_indices].sum()
            
            # è®¡ç®—å¹³å‡reward
            max_length = max(len(pred_bboxes), len(gt_bboxes))
            max_accuracy_reward = total_reward / max_length
            
    except Exception:
        pass
    return max_accuracy_reward

def vision_reasoner_non_repeat_reward(predict_str: str) -> float:
    non_repeat_reward = 1.0  # åˆå§‹æ»¡åˆ†
    try:
        sentences = predict_str.split('.')

        # ç§»é™¤ç©ºå¥å­
        sentences = [s.strip() for s in sentences if s.strip()]

        # æ£€æŸ¥é‡å¤
        seen = set()
        repeats = 0

        for sentence in sentences:
            if sentence in seen:
                repeats += 1
            if repeats >=2:
                non_repeat_reward = 0
                break
            seen.add(sentence)

    except Exception:
        pass

    return non_repeat_reward


def vision_reasoner_think_content_penalty(predict_str: str) -> float:
    """
    æ£€æŸ¥<think>æ ‡ç­¾ä¸­çš„å†…å®¹æ˜¯å¦åªåŒ…å«ç‰¹æ®Štoken
    å¦‚æœåªæœ‰ç‰¹æ®Štokenè€Œæ²¡æœ‰å®é™…æ–‡æœ¬å†…å®¹ï¼Œåˆ™æ‰£é™¤0.5åˆ†

    Args:
        predict_str: Model prediction string

    Returns:
        penalty: 0.0 (æ— æƒ©ç½š) æˆ– -0.5 (æœ‰æƒ©ç½š)
    """
    penalty = 0.0

    try:
        # æå–<think>æ ‡ç­¾ä¸­çš„å†…å®¹
        think_match = re.search(r'<think>(.*?)</think>', predict_str, re.DOTALL)
        if not think_match:
            return penalty

        think_content = think_match.group(1).strip()

        # å®šä¹‰æ‰€æœ‰ç‰¹æ®Štoken
        special_tokens = [
            '<|sam_pad|>', '<|depth_pad|>', '<|dino_pad|>', '<|pidinet_pad|>',
            '<|sd_pad|>', '<|intern_pad|>', '<|siglip_pad|>', '<|metaclip_pad|>'
        ]

        # ç§»é™¤æ‰€æœ‰ç‰¹æ®Štoken
        content_without_tokens = think_content
        for token in special_tokens:
            content_without_tokens = content_without_tokens.replace(token, '')

        # ç§»é™¤ç©ºç™½å­—ç¬¦
        content_without_tokens = content_without_tokens.strip()

        # å¦‚æœç§»é™¤ç‰¹æ®Štokenååªå‰©ä¸‹ç©ºç™½æˆ–æ ‡ç‚¹ç¬¦å·ï¼Œåˆ™æ–½åŠ æƒ©ç½š
        # ä¿ç•™ä¸€äº›å¸¸è§çš„è¿æ¥è¯å’ŒçŸ­è¯­
        if len(content_without_tokens) == 0:
            penalty = -0.5
        elif all(c in ' \n\t.,;:!?ã€ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ' for c in content_without_tokens):
            penalty = -0.5

    except Exception:
        pass

    return penalty


def vision_reasoner_token_context_penalty(predict_str: str) -> float:
    """
    æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦å‡ºç°åœ¨æ­£ç¡®çš„ä¸Šä¸‹æ–‡ä¸­ï¼š
    - <|sam_pad|> åº”è¯¥åªè·Ÿ 'segmentation' ç›¸å…³çš„è¯
    - <|depth_pad|> åº”è¯¥åªè·Ÿ 'depth' ç›¸å…³çš„è¯
    - <|dino_pad|> åº”è¯¥åªè·Ÿ 'perception feature' æˆ– 'feature' ç›¸å…³çš„è¯
    - <|pidinet_pad|> åº”è¯¥åªè·Ÿ 'edge' ç›¸å…³çš„è¯

    æ£€æŸ¥ç‰¹æ®Štokenä¹‹å‰çš„æ–‡æœ¬ï¼Œç¡®ä¿å‰é¢å‡ºç°çš„æ˜¯å¯¹åº”çš„æ­£ç¡®å…³é”®è¯
    å¦‚æœå‘ç°tokenåœ¨é”™è¯¯çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæ‰£é™¤0.5åˆ†

    Args:
        predict_str: Model prediction string

    Returns:
        penalty: 0.0 æˆ– -0.5
    """
    penalty = 0.0

    try:
        # æå–<think>æ ‡ç­¾ä¸­çš„å†…å®¹
        think_match = re.search(r'<think>(.*?)</think>', predict_str, re.DOTALL)
        if not think_match:
            return penalty

        think_content = think_match.group(1).lower()  # è½¬å°å†™ä»¥ä¾¿åŒ¹é…

        # å®šä¹‰tokenå’Œå¯¹åº”çš„æ­£ç¡®ä¸Šä¸‹æ–‡å…³é”®è¯
        token_context_rules = {
            '<|sam_pad|>': ['segmentation', 'segment', 'mask'],
            '<|depth_pad|>': ['depth', 'depth map'],
            '<|dino_pad|>': ['perception', 'perception feature', 'feature'],
            '<|pidinet_pad|>': ['edge', 'edge map']
        }

        has_violation = False

        for token, valid_keywords in token_context_rules.items():
            token_lower = token.lower()

            # æ£€æŸ¥tokenæ˜¯å¦å­˜åœ¨
            if token_lower not in think_content:
                continue

            # æŸ¥æ‰¾tokenå‡ºç°çš„ç¬¬ä¸€ä¸ªä½ç½®
            token_pos = think_content.find(token_lower)
            if token_pos == -1:
                continue

            # è·å–tokenä¹‹å‰çš„æ–‡æœ¬ (æœ€å¤šå‰100ä¸ªå­—ç¬¦ï¼Œç¡®ä¿èƒ½æ•è·åˆ°æè¿°è¯)
            context_start = max(0, token_pos - 20)
            preceding_text = think_content[context_start:token_pos]

            # æ£€æŸ¥å‰é¢çš„æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ä»»ä½•æœ‰æ•ˆå…³é”®è¯
            has_valid_keyword = any(keyword in preceding_text for keyword in valid_keywords)

            if not has_valid_keyword:
                has_violation = True
                break

        if has_violation:
            penalty = -1

    except Exception:
        pass

    return penalty


def vision_reasoner_token_count_penalty(predict_str: str) -> float:
    """
    æ£€æŸ¥ç‰¹æ®Štokençš„å‡ºç°æ¬¡æ•°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼š
    - <|sam_pad|> åº”è¯¥æ°å¥½å‡ºç°8æ¬¡
    - <|depth_pad|> åº”è¯¥æ°å¥½å‡ºç°4æ¬¡
    - <|dino_pad|> åº”è¯¥æ°å¥½å‡ºç°4æ¬¡
    - <|pidinet_pad|> åº”è¯¥æ°å¥½å‡ºç°4æ¬¡

    å¦‚æœä»»ä½•tokençš„å‡ºç°æ¬¡æ•°ä¸ç¬¦åˆè¦æ±‚ï¼Œæ‰£é™¤1åˆ†

    Args:
        predict_str: Model prediction string

    Returns:
        penalty: 0.0 æˆ– -1.0
    """
    penalty = 0.0

    try:
        # æå–<think>æ ‡ç­¾ä¸­çš„å†…å®¹
        think_match = re.search(r'<think>(.*?)</think>', predict_str, re.DOTALL)
        if not think_match:
            return penalty

        think_content = think_match.group(1)

        # å®šä¹‰æ¯ä¸ªtokençš„æœŸæœ›å‡ºç°æ¬¡æ•°
        expected_counts = {
            '<|sam_pad|>': 8,
            '<|depth_pad|>': 4,
            '<|dino_pad|>': 4,
            '<|pidinet_pad|>': 4
        }

        has_count_violation = False

        for token, expected_count in expected_counts.items():
            # è®¡ç®—tokençš„å®é™…å‡ºç°æ¬¡æ•°
            actual_count = think_content.count(token)

            # å¦‚æœæ¬¡æ•°ä¸åŒ¹é…ï¼Œæ–½åŠ æƒ©ç½š
            if actual_count != expected_count:
                has_count_violation = True
                break

        if has_count_violation:
            penalty = -1.0

    except Exception:
        pass

    return penalty


def vision_reasoner_token_phrase_matching_penalty(predict_str: str) -> float:
    """
    æ£€æŸ¥ç‰¹å®šçŸ­è¯­åé¢æ˜¯å¦ç´§è·Ÿç€æ­£ç¡®çš„ç‰¹æ®Štokenï¼š
    - "the segmentation of the image" åé¢åº”è¯¥æ°å¥½è·Ÿç€ 8ä¸ª <|sam_pad|>
    - "the depth map of the image" åé¢åº”è¯¥æ°å¥½è·Ÿç€ 4ä¸ª <|depth_pad|>
    - "the perception feature of the image" åé¢åº”è¯¥æ°å¥½è·Ÿç€ 4ä¸ª <|dino_pad|>

    å¦‚æœå‘ç°çŸ­è¯­åé¢è·Ÿçš„tokenä¸æ­£ç¡®ï¼Œæ‰£é™¤1åˆ†

    Args:
        predict_str: Model prediction string

    Returns:
        penalty: 0.0 æˆ– -1.0
    """
    penalty = 0.0

    try:
        # æå–<think>æ ‡ç­¾ä¸­çš„å†…å®¹
        think_match = re.search(r'<think>(.*?)</think>', predict_str, re.DOTALL)
        if not think_match:
            return penalty

        think_content = think_match.group(1)

        # å®šä¹‰çŸ­è¯­å’Œå¯¹åº”çš„æœŸæœ›tokenæ¨¡å¼
        phrase_token_rules = [
            {
                'phrase': r'the segmentation of the image\s+is\s+',
                'expected_token': '<|sam_pad|>',
                'expected_count': 8
            },
            {
                'phrase': r'the depth map of the image\s+is\s+',
                'expected_token': '<|depth_pad|>',
                'expected_count': 4
            },
            {
                'phrase': r'the perception feature of the image\s+is\s+',
                'expected_token': '<|dino_pad|>',
                'expected_count': 4
            },
            {
                'phrase': r'the edge map of the image\s+is\s+',
                'expected_token': '<|pidinet_pad|>',
                'expected_count': 4
            }

        ]

        has_violation = False

        for rule in phrase_token_rules:
            # æŸ¥æ‰¾çŸ­è¯­
            phrase_match = re.search(rule['phrase'], think_content, re.IGNORECASE)
            if not phrase_match:
                continue

            # è·å–çŸ­è¯­åé¢çš„å†…å®¹
            start_pos = phrase_match.end()
            remaining_text = think_content[start_pos:]

            # æœŸæœ›çš„tokenåºåˆ—
            expected_sequence = rule['expected_token'] * rule['expected_count']

            # æ£€æŸ¥æ˜¯å¦ä»¥æœŸæœ›çš„tokenåºåˆ—å¼€å¤´
            if not remaining_text.startswith(expected_sequence):
                has_violation = True
                break

            # æ£€æŸ¥æœŸæœ›åºåˆ—åé¢æ˜¯å¦ç´§è·Ÿç€å…¶ä»–tokenï¼ˆä¸åº”è¯¥æœ‰å¤šä½™çš„ç›¸åŒtokenï¼‰
            after_sequence = remaining_text[len(expected_sequence):]
            if after_sequence.startswith(rule['expected_token']):
                # å¦‚æœåé¢è¿˜æœ‰ç›¸åŒçš„tokenï¼Œè¯´æ˜æ•°é‡ä¸å¯¹
                has_violation = True
                break

        if has_violation:
            penalty = -1.0

    except Exception:
        pass

    return penalty

def vision_reasoner_compute_score(predict_str: str, ground_truth: str, verbose: bool = False) -> float:
    """
    Compute reward score for vision reasoner predictions.

    Args:
        predict_str: Model prediction string
        ground_truth: Ground truth annotations
        verbose: If True, print detailed reward breakdown
    """
    format_reward = vision_reasoner_format_reward(predict_str)
    accuracy_reward = vision_reasoner_accuracy_reward(predict_str, ground_truth)
    non_repeat_reward = vision_reasoner_non_repeat_reward(predict_str)

    # æ–°å¢çš„penalties
    think_content_penalty = vision_reasoner_think_content_penalty(predict_str)
    token_context_penalty = vision_reasoner_token_context_penalty(predict_str)
    token_count_penalty = vision_reasoner_token_count_penalty(predict_str)
    token_phrase_matching_penalty = vision_reasoner_token_phrase_matching_penalty(predict_str)

    # reward = format_reward + accuracy_reward + non_repeat_reward + think_content_penalty + token_context_penalty + token_count_penalty
    
    reward = format_reward + accuracy_reward  + non_repeat_reward + think_content_penalty + token_phrase_matching_penalty

    # Print detailed breakdown periodically
    if verbose and hasattr(vision_reasoner_compute_score, '_call_count'):
        vision_reasoner_compute_score._call_count += 1
    elif verbose:
        vision_reasoner_compute_score._call_count = 1

    if verbose and vision_reasoner_compute_score._call_count % 50 == 0:
        print("\n" + "="*60)
        print(f"ğŸ¯ REWARD BREAKDOWN (Call #{vision_reasoner_compute_score._call_count})")
        print("="*60)
        print(f"Format Reward:           {format_reward:.4f}")
        print(f"Accuracy Reward:         {accuracy_reward:.4f}")
        print(f"Non-repeat Reward:       {non_repeat_reward:.4f}")
        print(f"Think Content Penalty:   {think_content_penalty:.4f}")
        print(f"Token Context Penalty:   {token_context_penalty:.4f}")
        print(f"Token Count Penalty:     {token_count_penalty:.4f}")
        print(f"Total Reward:            {reward:.4f}")
        print(f"\nPrediction preview: {predict_str[:200]}...")
        print("="*60 + "\n")

    return reward

def batch_iou(boxes1, boxes2):
    # boxes1: (M,4), boxes2: (N,4)
    # å¹¿æ’­æœºåˆ¶è‡ªåŠ¨æ‰©å±•ç»´åº¦
    x11, y11, x12, y12 = np.split(boxes1, 4, axis=1)  # (M,1)
    x21, y21, x22, y22 = np.split(boxes2, 4, axis=1)  # (N,1)
    
    xA = np.maximum(x11, np.transpose(x21))  # (M,N)
    yA = np.maximum(y11, np.transpose(y21))
    xB = np.minimum(x12, np.transpose(x22))
    yB = np.minimum(y12, np.transpose(y22))
    
    interArea = np.maximum(0, xB - xA + 1) * np.maximum(0, yB - yA + 1)
    box1Area = (x12 - x11 + 1) * (y12 - y11 + 1)  # (M,1)
    box2Area = (x22 - x21 + 1) * (y22 - y21 + 1)  # (N,1)
    
    unionArea = box1Area + np.transpose(box2Area) - interArea
    iou = interArea / unionArea  # (M,N)
    return iou

def batch_l1_distance(boxes1, boxes2):
    # boxes1: (M,4), boxes2: (N,4)
    boxes1 = boxes1[:, np.newaxis, :]  # (M,1,4)
    boxes2 = boxes2[np.newaxis, :, :]  # (1,N,4)
    return np.mean(np.abs(boxes1 - boxes2), axis=2)  # (M,N)

def batch_points_distance(points1, points2):
    # points1: (M,2), points2: (N,2)
    points1 = points1[:, np.newaxis, :]  # (M,1,2)
    points2 = points2[np.newaxis, :, :]  # (1,N,2)
    
    # è®¡ç®—æ¬§æ°è·ç¦»
    dist = np.sqrt(np.sum((points1 - points2)**2, axis=2))  # (M,N)
    return dist

def batch_points_in_box(points, boxes):
    """
    æ£€æŸ¥æ¯ä¸ªç‚¹æ˜¯å¦åœ¨å¯¹åº”çš„æ¡†å†…
    points: (M,2) - Mä¸ªç‚¹çš„åæ ‡
    boxes: (M,4) - Mä¸ªæ¡†çš„åæ ‡ [x1,y1,x2,y2]
    è¿”å›: (M,) å¸ƒå°”æ•°ç»„
    """
    x_check = (points[:,0] >= boxes[:,0]) & (points[:,0] <= boxes[:,2])
    y_check = (points[:,1] >= boxes[:,1]) & (points[:,1] <= boxes[:,3])
    return x_check & y_check

if __name__ == "__main__":
    predict_str = """
<answer>
[{"bbox_2d": [10, 100, 398, 423], "point_2d": [283, 169]}]
</answer>
"""
    ground_truth = """
[{"bbox_2d": [416, 7, 833, 553], "point_2d": [648, 249]}]"""
    print(predict_str)
    print(ground_truth)
    print(vision_reasoner_compute_score(predict_str, ground_truth))
    