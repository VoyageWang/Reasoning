#!/usr/bin/env python3
"""
ILVR æ¨¡å‹æ¨ç† Demo

ç”¨æ³•:
    python inference_demo.py --model_dir /path/to/model --image /path/to/image.jpg --question "ä½ çš„é—®é¢˜"
"""
import os
import argparse
import torch
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor


def load_model_and_processor(model_dir: str, base_model_dir: str = None):
    """
    åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨

    Args:
        model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        base_model_dir: å¤‡ç”¨çš„åŸºç¡€æ¨¡å‹ç›®å½•ï¼ˆç”¨äºåŠ è½½ processorï¼‰

    Returns:
        model, processor
    """
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_dir}")

    # åŠ è½½ processor - ä¼˜å…ˆä»æœ¬åœ°åŠ è½½
    processor = None

    # å°è¯• 1: ä»æ¨¡å‹ç›®å½•åŠ è½½ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼‰
    try:
        processor = AutoProcessor.from_pretrained(
            model_dir,
            trust_remote_code=True,
            local_files_only=True
        )
        print("âœ“ Processor ä»æ¨¡å‹ç›®å½•åŠ è½½æˆåŠŸ")
    except Exception as e1:
        print(f"âš ï¸  ä»æ¨¡å‹ç›®å½•åŠ è½½å¤±è´¥: {e1}")

        # å°è¯• 2: ä»å¤‡ç”¨çš„åŸºç¡€æ¨¡å‹ç›®å½•åŠ è½½
        if base_model_dir and os.path.exists(base_model_dir):
            try:
                processor = AutoProcessor.from_pretrained(
                    base_model_dir,
                    trust_remote_code=True,
                    local_files_only=True
                )
                print(f"âœ“ Processor ä»åŸºç¡€æ¨¡å‹ç›®å½•åŠ è½½æˆåŠŸ: {base_model_dir}")
            except Exception as e2:
                print(f"âš ï¸  ä»åŸºç¡€æ¨¡å‹ç›®å½•åŠ è½½å¤±è´¥: {e2}")

        # å°è¯• 3: ä»å¸¸è§çš„æœ¬åœ°æ¨¡å‹è·¯å¾„åŠ è½½
        if processor is None:
            common_paths = [
                "/ytech_m2v_hdd/wangyuji/model/Qwen/Qwen2.5-VL-7B-Instruct",
                "/ytech_m2v_hdd/wangyuji/ckpt/ilvr_final/first",
            ]

            for path in common_paths:
                if os.path.exists(path):
                    try:
                        processor = AutoProcessor.from_pretrained(
                            path,
                            trust_remote_code=True,
                            local_files_only=True
                        )
                        print(f"âœ“ Processor ä»å¤‡ç”¨è·¯å¾„åŠ è½½æˆåŠŸ: {path}")
                        break
                    except Exception:
                        continue

        # æœ€åå°è¯•ï¼šä¸ä½¿ç”¨ local_files_onlyï¼ˆéœ€è¦ç½‘ç»œï¼‰
        if processor is None:
            print("âš ï¸  æ‰€æœ‰æœ¬åœ°åŠ è½½å°è¯•å¤±è´¥ï¼Œå°è¯•ä» HuggingFace Hub ä¸‹è½½...")
            try:
                processor = AutoProcessor.from_pretrained(
                    "Qwen/Qwen2.5-VL-7B-Instruct",
                    trust_remote_code=True
                )
                print("âœ“ Processor ä» HuggingFace Hub ä¸‹è½½æˆåŠŸ")
            except Exception as e:
                raise RuntimeError(
                    f"âŒ æ— æ³•åŠ è½½ processorï¼\n"
                    f"   è¯·æ£€æŸ¥:\n"
                    f"   1. æ¨¡å‹ç›®å½•æ˜¯å¦åŒ…å« processor æ–‡ä»¶\n"
                    f"   2. åŸºç¡€æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨\n"
                    f"   3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                    f"   é”™è¯¯: {e}"
                )

    # åŠ è½½æ¨¡å‹
    try:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_dir,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            attn_implementation="flash_attention_2",
            local_files_only=True
        )
    except Exception as e:
        print(f"âš ï¸  Flash Attention 2 åŠ è½½å¤±è´¥ ({e})ï¼Œä½¿ç”¨ eager æ¨¡å¼...")
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_dir,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            attn_implementation="eager",
            local_files_only=True
        )

    model.eval()

    # å¯ç”¨ TF32ï¼ˆå¦‚æœæ”¯æŒï¼‰
    try:
        torch.backends.cuda.matmul.allow_tf32 = True
    except Exception:
        pass

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, processor


def inference(
    model,
    processor,
    image_path: str,
    question: str,
    max_new_tokens: int = 512,
    temperature: float = 0.0,
    top_p: float = 1.0
):
    """
    æ‰§è¡Œæ¨ç†

    Args:
        model: æ¨¡å‹
        processor: å¤„ç†å™¨
        image_path: å›¾ç‰‡è·¯å¾„
        question: é—®é¢˜
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: æ¸©åº¦å‚æ•°
        top_p: top-p é‡‡æ ·å‚æ•°

    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    # åŠ è½½å›¾ç‰‡
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")

    image = Image.open(image_path).convert("RGB")
    print(f"ğŸ–¼ï¸  å›¾ç‰‡: {image_path} ({image.size})")
    print(f"â“ é—®é¢˜: {question}")

    # æ„å»ºå¯¹è¯
    content = [
        {"type": "image"},
        {"type": "text", "text": question}
    ]

    conversations = [
        {"role": "user", "content": content}
    ]

    # åº”ç”¨ chat template
    prompt = processor.apply_chat_template(
        conversations,
        tokenize=False,
        add_generation_prompt=True
    )

    # å¤„ç†è¾“å…¥
    inputs = processor(
        text=[prompt],
        images=[image],
        return_tensors="pt",
        padding=True
    )

    # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    inputs = {k: v.to(model.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}

    # ç”Ÿæˆé…ç½®
    gen_kwargs = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "do_sample": (temperature > 0)
    }

    print("\nğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")

    # ç”Ÿæˆ
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            **gen_kwargs,
        )

    # è§£ç è¾“å‡º
    output_text = processor.batch_decode(
        output_ids,
        skip_special_tokens=False
    )[0].strip()

    return output_text


def extract_answer(output_text: str) -> str:
    """
    ä»è¾“å‡ºä¸­æå–ç­”æ¡ˆéƒ¨åˆ†

    Args:
        output_text: å®Œæ•´è¾“å‡º

    Returns:
        æå–çš„ç­”æ¡ˆ
    """
    import re

    # æå– assistant éƒ¨åˆ†
    match = re.search(
        r"<\|im_start\|>\s*assistant\s*(.*?)(?:<\|im_end\|>|$)",
        output_text,
        flags=re.S | re.I
    )

    if match:
        assistant_content = match.group(1)
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ¨¡å¼
        match = re.search(
            r"<\|assistant\|>\s*(.*?)(?:<\|im_end\|>|<\|endoftext\|>|$)",
            output_text,
            flags=re.S | re.I
        )
        if match:
            assistant_content = match.group(1)
        else:
            assistant_content = output_text

    # æ¸…ç†ç‰¹æ®Š tokens
    special_tokens = [
        "<|im_end|>", "<|endoftext|>", "<|eot_id|>",
        "<|latent_start|>", "<|latent_end|>", "<|latent_pad|>",
        "<|vision_start|>", "<|vision_end|>", "<|image_pad|>"
    ]

    for token in special_tokens:
        assistant_content = assistant_content.replace(token, "")

    return assistant_content.strip()


def main():
    parser = argparse.ArgumentParser(
        description="ILVR æ¨¡å‹æ¨ç† Demo",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•å¼ å›¾ç‰‡æ¨ç†
  python inference_demo.py \\
    --model_dir /ytech_m2v_hdd/wangyuji/ckpt/ilvr_final/first \\
    --image /path/to/image.jpg \\
    --question "What is in this image?"

  # è°ƒæ•´ç”Ÿæˆå‚æ•°
  python inference_demo.py \\
    --model_dir /path/to/model \\
    --image /path/to/image.jpg \\
    --question "Describe this image in detail" \\
    --max_new_tokens 1024 \\
    --temperature 0.7
        """
    )

    parser.add_argument(
        "--model_dir",
        type=str,
        default="/ytech_m2v_hdd/wangyuji/ckpt/ilvr_final/covt_ilvr",
        help="æ¨¡å‹ç›®å½•è·¯å¾„ (é»˜è®¤: /ytech_m2v_hdd/wangyuji/ckpt/ilvr_final/first)"
    )
    parser.add_argument(
        "--base_model_dir",
        type=str,
        default="/ytech_m2v_hdd/wangyuji/model/Qwen/Qwen2.5-VL-7B-Instruct",
        help="åŸºç¡€æ¨¡å‹ç›®å½•ï¼Œç”¨äºåŠ è½½ processor (é»˜è®¤: /ytech_m2v_hdd/wangyuji/model/Qwen/Qwen2.5-VL-7B-Instruct)"
    )
    parser.add_argument(
        "--image",
        type=str,
        default="/ytech_m2v_hdd/wangyuji/data/JingkunAn/RefSpatial/2D/image/000d32e2b7d81b8c.jpg",
        help="è¾“å…¥å›¾ç‰‡è·¯å¾„ (é»˜è®¤: /ytech_m2v_hdd/wangyuji/data/comt/images_comt/creation/12035.png)"
    )
    parser.add_argument(
        "--question",
        type=str,
        default="Does white stone cross at center appear under smooth stone cross at center? (A) yes (B) no",
        help="é—®é¢˜æ–‡æœ¬ (é»˜è®¤: What is in this image?)" # Yes, point (0.032, 0.512) is closer to viewer.
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆ token æ•° (é»˜è®¤: 512)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="æ¸©åº¦å‚æ•° (é»˜è®¤: 0.0, ç¡®å®šæ€§ç”Ÿæˆ)"
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=1.0,
        help="Top-p é‡‡æ ·å‚æ•° (é»˜è®¤: 1.0)"
    )
    parser.add_argument(
        "--show_full",
        action="store_true",
        help="æ˜¾ç¤ºå®Œæ•´è¾“å‡ºï¼ˆåŒ…æ‹¬ç‰¹æ®Š tokensï¼‰"
    )

    args = parser.parse_args()

    print("="*60)
    print("ğŸš€ ILVR æ¨¡å‹æ¨ç† Demo")
    print("="*60)

    # åŠ è½½æ¨¡å‹
    model, processor = load_model_and_processor(args.model_dir, args.base_model_dir)

    # æ¨ç†
    output = inference(
        model=model,
        processor=processor,
        image_path=args.image,
        question=args.question,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_p=args.top_p
    )

    # æ˜¾ç¤ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ“ æ¨¡å‹è¾“å‡º")
    print("="*60)

    if args.show_full:
        print("\nã€å®Œæ•´è¾“å‡ºã€‘:")
        print(output)
        print("\n" + "-"*60)

    # æå–å¹¶æ˜¾ç¤ºç­”æ¡ˆ
    answer = extract_answer(output)
    print("\nã€æå–çš„ç­”æ¡ˆã€‘:")
    print(answer)

    print("\n" + "="*60)
    print("âœ… å®Œæˆ")
    print("="*60)


if __name__ == "__main__":
    main()
