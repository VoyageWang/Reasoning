#!/usr/bin/env python3
"""
å°†æ¨¡å‹è½¬æ¢ä¸º HuggingFace safetensors æ ¼å¼

æ”¯æŒä¸¤ç§è¾“å…¥:
1. DeepSpeed checkpoint (åŒ…å« zero_to_fp32.py)
2. å·²æœ‰ pytorch_model.bin çš„ç›®å½•
"""
import os
import sys
import argparse
import subprocess
import shutil
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor


def is_deepspeed_checkpoint(checkpoint_dir: str) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦ä¸º DeepSpeed checkpoint

    Args:
        checkpoint_dir: checkpoint ç›®å½•è·¯å¾„

    Returns:
        True if DeepSpeed checkpoint, False otherwise
    """
    zero_to_fp32_script = os.path.join(checkpoint_dir, "zero_to_fp32.py")
    global_step_dirs = [d for d in os.listdir(checkpoint_dir)
                       if d.startswith("global_step") and os.path.isdir(os.path.join(checkpoint_dir, d))]

    return os.path.exists(zero_to_fp32_script) or len(global_step_dirs) > 0


def convert_deepspeed_checkpoint(
    checkpoint_dir: str,
    output_dir: str,
    remove_temp: bool = True
):
    """
    å°† DeepSpeed checkpoint è½¬æ¢ä¸º pytorch_model æ ¼å¼

    Args:
        checkpoint_dir: DeepSpeed checkpoint ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        remove_temp: æ˜¯å¦åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    """
    print("\nğŸ”§ æ£€æµ‹åˆ° DeepSpeed checkpointï¼Œå¼€å§‹è½¬æ¢...")

    zero_to_fp32_script = os.path.join(checkpoint_dir, "zero_to_fp32.py")
    if not os.path.exists(zero_to_fp32_script):
        raise FileNotFoundError(f"æœªæ‰¾åˆ° zero_to_fp32.py: {zero_to_fp32_script}")

    # Step 1: ä½¿ç”¨ zero_to_fp32.py è½¬æ¢
    print("\næ­¥éª¤ 1/4: è½¬æ¢ DeepSpeed checkpoint ä¸º pytorch_model...")
    temp_model_path = os.path.join(output_dir, "pytorch_model.bin")

    cmd = [sys.executable, zero_to_fp32_script, checkpoint_dir, temp_model_path]
    print(f"æ‰§è¡Œ: {' '.join(cmd)}")

    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"âŒ è½¬æ¢å¤±è´¥ï¼")
        print(f"é”™è¯¯: {result.stderr}")
        raise RuntimeError("DeepSpeed checkpoint è½¬æ¢å¤±è´¥")

    print("âœ“ æˆåŠŸç”Ÿæˆ pytorch_model")

    # å¦‚æœ zero_to_fp32.py åˆ›å»ºäº†ç›®å½•ï¼Œç§»åŠ¨æ–‡ä»¶
    if os.path.isdir(temp_model_path):
        print("æ£€æµ‹åˆ°ç›®å½•æ ¼å¼ï¼Œæ­£åœ¨ç§»åŠ¨æ–‡ä»¶...")
        for item in os.listdir(temp_model_path):
            src = os.path.join(temp_model_path, item)
            dst = os.path.join(output_dir, item)
            shutil.move(src, dst)
            print(f"  ç§»åŠ¨: {item}")
        os.rmdir(temp_model_path)

    # Step 2: å¤åˆ¶é…ç½®æ–‡ä»¶
    print("\næ­¥éª¤ 2/4: å¤åˆ¶é…ç½®æ–‡ä»¶å’Œ tokenizer...")
    files_to_copy = [
        "config.json", "generation_config.json",
        "tokenizer.json", "tokenizer_config.json",
        "vocab.json", "merges.txt",
        "special_tokens_map.json", "added_tokens.json"
    ]

    for filename in files_to_copy:
        src = os.path.join(checkpoint_dir, filename)
        if os.path.exists(src):
            dst = os.path.join(output_dir, filename)
            shutil.copy2(src, dst)
            print(f"  âœ“ {filename}")


def convert_to_safetensors(
    model_dir: str,
    output_dir: str = None,
    remove_pytorch: bool = True
):
    """
    åŠ è½½æ¨¡å‹å¹¶ä¿å­˜ä¸º safetensors æ ¼å¼

    Args:
        model_dir: æ¨¡å‹ç›®å½•ï¼ˆåŒ…å« pytorch_model æˆ–å·²æ˜¯è¾“å‡ºç›®å½•ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™åœ¨åŸåœ°è½¬æ¢ï¼‰
        remove_pytorch: æ˜¯å¦åˆ é™¤ pytorch_model æ–‡ä»¶
    """
    if output_dir is None:
        output_dir = model_dir

    print("\næ­¥éª¤ 3/4: åŠ è½½æ¨¡å‹å¹¶ä¿å­˜ä¸º safetensors...")
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")

    try:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            output_dir,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")

        print("\nä¿å­˜ä¸º safetensors æ ¼å¼...")
        model.save_pretrained(
            output_dir,
            safe_serialization=True,
            max_shard_size="5GB"
        )
        print("âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ")

        # ä¿å­˜ processor
        try:
            processor = AutoProcessor.from_pretrained(model_dir, trust_remote_code=True)
            processor.save_pretrained(output_dir)
            print("âœ“ Processor å·²ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸  Processor ä¿å­˜å¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½/ä¿å­˜å¤±è´¥: {e}")
        raise

    # Step 4: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if remove_pytorch:
        print("\næ­¥éª¤ 4/4: æ¸…ç† pytorch_model æ–‡ä»¶...")
        for f in os.listdir(output_dir):
            if f.startswith("pytorch_model"):
                fpath = os.path.join(output_dir, f)
                if os.path.isfile(fpath):
                    os.remove(fpath)
                    print(f"  ğŸ—‘ï¸  åˆ é™¤: {f}")
    else:
        print("\næ­¥éª¤ 4/4: è·³è¿‡æ¸…ç†ï¼ˆä¿ç•™ pytorch_model æ–‡ä»¶ï¼‰")

    # æ˜¾ç¤ºæœ€ç»ˆæ–‡ä»¶
    print("\nâœ… è½¬æ¢å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("\næœ€ç»ˆæ–‡ä»¶åˆ—è¡¨:")

    for f in sorted(os.listdir(output_dir)):
        fpath = os.path.join(output_dir, f)
        if os.path.isfile(fpath):
            size = os.path.getsize(fpath) / (1024**3)
            icon = "ğŸ”’" if f.endswith(".safetensors") else "ğŸ“„"
            print(f"  {icon} {f} ({size:.2f} GB)")


def main():
    parser = argparse.ArgumentParser(
        description="å°†æ¨¡å‹è½¬æ¢ä¸º safetensors æ ¼å¼ï¼ˆè‡ªåŠ¨æ£€æµ‹è¾“å…¥ç±»å‹ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # DeepSpeed checkpoint è½¬æ¢
  python convert_to_safetensors.py \\
    --checkpoint_dir /path/to/checkpoint-810 \\
    --output_dir /path/to/output

  # åŸåœ°è½¬æ¢å·²æœ‰çš„ pytorch_model
  python convert_to_safetensors.py \\
    --checkpoint_dir /path/to/model_with_pytorch_bin

  # ä¿ç•™ pytorch_model æ–‡ä»¶
  python convert_to_safetensors.py \\
    --checkpoint_dir /path/to/checkpoint \\
    --output_dir /path/to/output \\
    --keep_pytorch
        """
    )

    parser.add_argument(
        "--checkpoint_dir",
        type=str,
        required=True,
        help="Checkpoint ç›®å½•è·¯å¾„ï¼ˆDeepSpeed æˆ– pytorch_modelï¼‰"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼šåŸåœ°è½¬æ¢ï¼‰"
    )
    parser.add_argument(
        "--keep_pytorch",
        action="store_true",
        help="ä¿ç•™ pytorch_model æ–‡ä»¶"
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.checkpoint_dir):
        print(f"âŒ é”™è¯¯: Checkpoint ç›®å½•ä¸å­˜åœ¨: {args.checkpoint_dir}")
        sys.exit(1)

    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir is None:
        args.output_dir = args.checkpoint_dir
        print(f"â„¹ï¸  æœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œå°†åŸåœ°è½¬æ¢: {args.output_dir}")
    else:
        os.makedirs(args.output_dir, exist_ok=True)

    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹æ¨¡å‹è½¬æ¢")
    print(f"{'='*60}")
    print(f"è¾“å…¥: {args.checkpoint_dir}")
    print(f"è¾“å‡º: {args.output_dir}")
    print(f"{'='*60}")

    # è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢
    if is_deepspeed_checkpoint(args.checkpoint_dir):
        print("\nğŸ“¦ æ£€æµ‹ç±»å‹: DeepSpeed Checkpoint")
        convert_deepspeed_checkpoint(
            args.checkpoint_dir,
            args.output_dir,
            remove_temp=True
        )
    else:
        print("\nğŸ“¦ æ£€æµ‹ç±»å‹: PyTorch Model (å·²æœ‰ pytorch_model)")

    # è½¬æ¢ä¸º safetensors
    convert_to_safetensors(
        args.checkpoint_dir,
        args.output_dir,
        remove_pytorch=not args.keep_pytorch
    )

    print(f"\n{'='*60}")
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
